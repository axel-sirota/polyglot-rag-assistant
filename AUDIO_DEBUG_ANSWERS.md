# Answers to Audio Debugging Questions

## 1. Audio Verification in Browser

To check audio activity when agent is "speaking":

### Browser Volume Mixer Check
- Open browser developer tools (F12)
- Go to the Network tab and filter by WebRTC
- When agent is in "speaking" state, check if you see any audio activity in your OS volume mixer
- In Chrome: chrome://webrtc-internals/ shows detailed stats

### WebRTC Stats for Audio Levels
Run this in browser console:
```javascript
// Check inbound audio stats from agent
const pc = room.peerConnections[0]; // Get peer connection
const stats = await pc.getStats();
stats.forEach(stat => {
    if (stat.type === 'inbound-rtp' && stat.kind === 'audio') {
        console.log('Inbound audio:', {
            audioLevel: stat.audioLevel, // 0-1, where 1 is loudest
            bytesReceived: stat.bytesReceived,
            packetsReceived: stat.packetsReceived,
            packetsLost: stat.packetsLost
        });
    }
});
```

### Browser Audio Output Test
```javascript
// Test browser audio is working
const audio = new Audio('https://www.soundhelix.com/examples/mp3/SoundHelix-Song-1.mp3');
audio.play(); // Should hear music if audio works
```

## 2. Agent Participant Tracks

Use the debug script I created (`check-agent-audio.js`) or run this directly:

```javascript
// Find agent and check tracks
const agent = Array.from(room.participants.values()).find(p => p.identity.startsWith('agent-'));
if (agent) {
    console.log('Agent tracks:', agent.tracks);
    console.log('Audio tracks:', Array.from(agent.tracks.values()).filter(t => t.kind === 'audio'));
    
    // Check each audio track
    agent.tracks.forEach((track, sid) => {
        if (track.kind === 'audio') {
            console.log(`Audio track ${sid}:`, {
                subscribed: track.subscribed,
                muted: track.muted,
                enabled: track.mediaStreamTrack?.enabled,
                readyState: track.mediaStreamTrack?.readyState
            });
        }
    });
}
```

## 3. Working Reference Investigation

### Last Known Working State
Based on git history, the agent was working after commit `85536ea` (July 5, 2025) which:
- Implemented new AgentSession pattern
- Added prewarm function
- Used AutoSubscribe.AUDIO_ONLY
- Had fallback from OpenAI Realtime to STT-LLM-TTS

### What Changed Since Then
Looking at subsequent commits:
1. `e437edf` - Fixed async callback error
2. `185f23a` - Updated agent name configuration 
3. `67d2951` - Fixed LiveKit URL mismatch
4. `e8dda17` - Added event handlers for speech transcription
5. `930636a` - **Removed `session.say()` call that was failing**

### Critical Change Found
In the latest commit, we removed an initial greeting that used `session.say()`:
```python
# This was removed because it failed with OpenAI Realtime
await session.say("Hello! I'm your flight search assistant.", allow_interruptions=True)
```

The error was: "trying to generate speech from text without a TTS model"

## Additional Debugging Steps

### 1. Check OpenAI Realtime Connection
Add logging to verify OpenAI connection:
```python
@session.on("realtime_connected")
def on_realtime_connected(event):
    logger.info("OpenAI Realtime WebSocket connected")

@session.on("realtime_error") 
def on_realtime_error(event):
    logger.error(f"OpenAI Realtime error: {event}")
```

### 2. Force Fallback to STT-LLM-TTS
Temporarily disable OpenAI Realtime to test:
```python
# Comment out the Realtime model and force fallback
session = AgentSession(
    vad=vad,
    stt=deepgram.STT(model="nova-3"),
    llm=openai.LLM(model="gpt-4o-mini"),
    tts=cartesia.TTS(),  # or openai.TTS()
    turn_detection="vad"
)
```

### 3. Monitor Audio Track Publishing
Add this event handler:
```python
@ctx.room.on("local_track_published")
def on_local_track_published(publication: rtc.LocalTrackPublication):
    logger.info(f"ðŸ“¡ Local track published: {publication.kind} - SID: {publication.sid}")
    if publication.kind == rtc.TrackKind.KIND_AUDIO:
        logger.info("âœ… Audio track successfully published!")
```

## Hypothesis

The issue likely stems from:
1. **OpenAI Realtime model not generating audio** - The WebSocket might be connected but not producing audio
2. **Missing audio track publication** - AgentSession might not be automatically publishing the audio track
3. **Audio codec mismatch** - Browser might not support the audio format from OpenAI Realtime

## Immediate Actions

1. Run the browser debug script when agent is connected
2. Test with fallback STT-LLM-TTS pipeline (disable Realtime)
3. Add the suggested event handlers to track audio publishing
4. Check OpenAI API key has access to Realtime API
5. Monitor browser console for any WebRTC errors